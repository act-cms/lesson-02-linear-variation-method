{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90d67ce-e6f4-4837-bf13-853d796cb4b9",
   "metadata": {},
   "source": [
    "# APPROXIMATE SOLUTION OF THE SCHRÖDINGER EQUATION FOR A MODIFIED SQUARE WELL POTENTIAL - STUDENT HANDOUT  #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daecc60e-78fd-4b15-ac9e-a02983d2c57c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Learning outcomes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a7b9855-aad6-4466-a216-c75e202ed122",
   "metadata": {},
   "source": [
    "After completing these exercises, you will be able to\n",
    "\n",
    "- describe key aspects of the linear variation method\n",
    "- apply the linear variational method to obtain approximate solutions of the time-independent Schrödinger equation\n",
    "- explain the cost to benefit ratio of accuracy versus computational complexity\n",
    "- demonstrate skills for debugging code designed for scientific applications\n",
    "- access and modify elements of arrays and/or matrices\n",
    "- use scipy library functions solve eigenvalue equations\n",
    "- visualize the solutions of the time-independent Schrödinger equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe119cee-b0b9-4ead-bc05-a43331bc4a31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <span style=\"color:blue\"> Pre-lab activities </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8f9fa-6774-4afc-a04d-7fd17c599233",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "Before coming to lab, please complete the following and **turn in your solutions to problems 2 - 7 at the beginning of the lab period**.\n",
    "\n",
    "1. Read sections 0 through 4. *If you have some background in linear algebra (i.e. you know how to expand the determinant of a matrix or how to calculate a matrix vector product), you may skip section 0. If you want to see a detailed application of the linear variation method to a \"simple\" problem involving two basis functions, please see the appendix at the end of this notebook*\n",
    "   \n",
    "2. Provide a concise (3-4 sentence) summary of the basic tenets of the linear variation method.\n",
    "   \n",
    "3. Suppose you were to use the linear variation method with $N=4$ basis functions $\\phi_n$ (where $\\phi_n$ are particle in a box eigenfunctions) to calculate the ground-state energy for an electron that experiences the potential given in problem 4 above. Would you expect that your result for the ground-state energy is above or below the energy with two basis functions? Would the approximate ground-state energy be above or below the exact ground-state energy\n",
    "\n",
    "4. Given that the particle in a box eigenfunctions $\\phi_n(x) = \\sqrt{\\frac{2}{L}} {\\rm sin} \\left( \\frac{n\\pi x}{L} \\right)$ are orthonormal eigenfunctions of the kinetic energy operator $\\hat{T}_x = -\\frac{\\hbar^2}{2m} \\frac{{\\rm d}^2}{{\\rm d}x^2}$, show that\n",
    "\\begin{equation}\n",
    "T_{n,m} = \\int_0^L \\phi_n(x)^* \\hat{T}_x \\phi_m(x) {\\rm d}x = \n",
    "\\begin{cases}\n",
    "0 \\hspace{1.7cm} n \\neq m\\\\\n",
    "\\\\\n",
    "n^2 \\frac{\\hbar^2\\pi^2}{2mL^2} \\hspace{0.55cm} n = m\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "5. A particle confined to a box of width $L$ experiences the potential\n",
    "\\begin{equation}\n",
    "V(x) = \\begin{cases}\n",
    " 0 \\hspace{2.1cm} 0 \\leq x \\leq L/2 \\\\\n",
    " \\\\\n",
    " V_0 \\hspace{2cm} L/2 \\leq x \\leq L \\\\\n",
    " \\\\\n",
    " \\infty \\hspace{2cm} \\text{everywhere else}\n",
    "\\end{cases} \\tag{2.1}\n",
    "\\end{equation}\n",
    "   Using the particle in a box eigenfunctions $\\phi_n(x) = \\sqrt{\\frac{2}{L}} {\\rm sin} \\left( \\frac{n\\pi x}{L} \\right)$, show that\n",
    "\\begin{equation}\n",
    "V_{1,2} = \\int_0^L \\phi_1(x)^* \\hat{V}\\phi_{2}(x) {\\rm d}x = -4.24 \\hspace{1mm} {\\rm eV}\n",
    "\\end{equation}\n",
    "The integral\n",
    "\\begin{equation}\n",
    "\\int {\\rm sin}(Ax){\\rm sin}(Bx) dx =\\frac{1}{2} \\left( \\frac{{\\rm sin} [(A-B)x]}{A-B} - \\frac{{\\rm sin} [(A+B)x]}{A+B} \\right) + C \n",
    "\\end{equation}\n",
    "may be useful here.\n",
    "  \n",
    "6. Show that the two vectors ${\\bf c}^{(1)} = \n",
    "\\begin{pmatrix}\n",
    " 0.99860 \\\\ - 0.03747\n",
    "\\end{pmatrix}$ and ${\\bf c}^{(2)} = \n",
    "\\begin{pmatrix}\n",
    "- 0.03747 \\\\  0.99860\n",
    "\\end{pmatrix}$ are orthogonal and normalized. \n",
    "\n",
    "7. Consider $V(x)$ given at the beginning of [section 6](#assignment). Write down the integral expression(s) you will need to evaluate for computing the matrix element $V_{m,n}$ of the potential energy matrix $\\bf V$. Include the appropriate limits of integration and other factors such as $V_1$ and $V_B$. You do not need to solve the integrals, however, you should think about how you would apply your work from section 13 of the [\"Introduction to programming and Python\"](https://github.com/act-cms/lesson-01-introduction-to-programming-and-python) exercises to numerically calculate the integrals.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004377f-e332-4e18-a54b-50c1a8f3be2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 0. Review of useful properties of vectors, matrices, determinants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6468bea-2cac-487d-9c56-fafeadd3f511",
   "metadata": {},
   "source": [
    "This section provides a pragmatic review of properties of matrices and vectors. For a more in-depth discussion, please see a textbook on linear algebra.\n",
    "\n",
    "<br> <center> **MATRICES** </center> <br>\n",
    "\n",
    "An $N\\times M$ matrix ${\\bf A}$ is a 2-dimensional collection of $N\\cdot M$ numbers\n",
    "\\begin{equation}\n",
    "{\\bf A} = \n",
    "\\begin{pmatrix}\n",
    "A_{1,1} & A_{1,2} & \\dots  & A_{1,M} \\\\\n",
    "A_{2,1} & A_{2,2} & \\dots  & A_{2,M} \\\\\n",
    "\\vdots & \\vdots   & \\ddots & \\vdots  \\\\\n",
    "A_{N,1} & A_{N,2} & \\dots  & A_{N,M}\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "The transpose of a matrix (denoted by $^T$) is obtained by interchanging columns and rows. Thus\n",
    "\\begin{equation}\n",
    "{\\bf A}^T = \n",
    "\\begin{pmatrix}\n",
    "A_{1,1} & A_{2,1} & \\dots  & A_{M,1} \\\\\n",
    "A_{1,2} & A_{2,2} & \\dots  & A_{M,2} \\\\\n",
    "\\vdots & \\vdots   & \\ddots & \\vdots  \\\\\n",
    "A_{1,N} & A_{2,N} & \\dots  & A_{M,N}\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "and the transpose of an $N\\times M$ matrix is an $M\\times N$ matrix. When $N=M$, the matrix is said to be a square matrix. Furthermore, if ${\\bf A} = {\\bf A}^{T}$ (i.e. $A_{n,m} = A_{m,n}$) the matrix ${\\bf A}$ is said to be symmetric. A square matrix $\\bf D$ is said to be diagonal when all off-diagonal elements ($D_{n,m}$ with $n\\neq m$) are zero\n",
    "\\begin{equation}\n",
    "{\\bf D} = \n",
    "\\begin{pmatrix}\n",
    "D_{1,1} &     0    & \\dots   & 0 \\\\\n",
    " 0      & D_{2,2}  & \\dots   & 0 \\\\\n",
    "\\vdots  & \\vdots   & \\ddots  & \\vdots  \\\\\n",
    " 0      & 0        & \\dots   & D_{N,N}\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The identity matrix $\\bf I$ is a special case of a diagonal matrix in which all diagonal elements are equal to 1\n",
    "\\begin{equation}\n",
    "{\\bf I} = \n",
    "\\begin{pmatrix}\n",
    " 1      &     0    & \\dots   & 0 \\\\\n",
    " 0      &     1    & \\dots   & 0 \\\\\n",
    "\\vdots  & \\vdots   & \\ddots  & \\vdots  \\\\\n",
    " 0      & 0        & \\dots   & 1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "A matrix $\\bf A$ is said to invertible if there exists a matrix ${\\bf A}^{-1}$ (called the inverse of ${\\bf A}$) such that ${\\bf A}^{-1} \\bf A = I$. A matrix is invertible if its determinant (see below) does not vanish.\n",
    "\n",
    "<br> <center> **VECTORS** </center> <br>\n",
    "\n",
    "A column vector ${\\bf c}$ is a 1-dimensional collection of $N$ numbers\n",
    "\\begin{equation}\n",
    "{\\bf c} = \n",
    "\\begin{pmatrix}\n",
    "c_1 \\\\\n",
    "c_2 \\\\\n",
    "\\vdots \\\\\n",
    "c_N\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "where $c_i$ is the i$^{\\rm th}$ element of the vector. Alternatively, we can view a colum vector as an $N \\times 1$ matrix. The transpose of a column vector is a row vector (or a $1\\times N$ matrix). The zero vector $\\bf 0$ is an array of zeros. \n",
    "\n",
    "<br> <center> **DOT/SCALAR/INNER PRODUCTS OF VECTORS** </center> <br>\n",
    "\n",
    "The dot product (a.k.a. inner product) of a row vector $\\bf a$ with a colum vector $\\bf b$ is a scalar\n",
    "\\begin{equation}\n",
    "{\\bf a \\cdot b} = \n",
    "\\begin{pmatrix}\n",
    "a_1 & a_2 & \\dots & a_N\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_N\n",
    "\\end{pmatrix}=\n",
    "a_1b_1 + a_2b_2 + \\dots +a_Nb_N = \\sum_{n}^N a_nb_n\n",
    "\\end{equation}\n",
    "Note that for the dot product to be well-defined, the length of the two vectors must be the same. As an example, the dot product of\n",
    "\\begin{equation}\n",
    "{\\bf a} = \n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 2  \\\\\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}\n",
    "{\\bf b} = \n",
    "\\begin{pmatrix}\n",
    "3 \\\\\n",
    "4 \\\\\n",
    "5\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "is\n",
    "\\begin{equation}\n",
    "{\\bf c} = \n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 2  \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "3 \\\\\n",
    "4 \\\\\n",
    "5\n",
    "\\end{pmatrix} =\n",
    "0 \\cdot 3 + 1 \\cdot 4 + 3 \\cdot 5 = 19\n",
    "\\end{equation}\n",
    "The norm $\\vert\\vert {\\bf c} \\vert \\vert $ of a vector $\\bf c$ is $\\vert\\vert {\\bf c} \\vert \\vert = \\sqrt{{\\bf c \\cdot c}}$.\n",
    "\n",
    "<br> <center> **MATRIX VECTOR PRODUCTS** </center> <br>\n",
    "\n",
    "The product of an $N\\times M$ matrix $\\bf A$ with a $M\\times 1$ column vector $\\bf b$ is an $N\\times 1$ column vector $\\bf c$ with elements\n",
    "\\begin{equation}\n",
    "c_n = \\sum_m^M A_{n,m} b_m\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "{\\bf Ab} = \n",
    "\\begin{pmatrix}\n",
    "A_{1,1} & A_{1,2} & \\dots  & A_{1,M} \\\\\n",
    "A_{2,1} & A_{2,2} & \\dots  & A_{2,M} \\\\\n",
    "\\vdots & \\vdots   & \\ddots & \\vdots  \\\\\n",
    "\\vdots & \\vdots   & \\ddots & \\vdots  \\\\\n",
    "A_{N,1} & A_{N,2} & \\dots  & A_{N,M}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_M\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "A_{1,1}b_1 + A_{1,2}b_2 + \\dots + A_{1,M}b_M \\\\\n",
    "A_{2,1}b_1 + A_{2,2}b_2 + \\dots + A_{2,M}b_M \\\\\n",
    "\\vdots \\\\\n",
    "\\vdots \\\\\n",
    "A_{N,1}b_1 + A_{N,2}b_2 + \\dots + A_{N,M}b_M\n",
    "\\end{pmatrix} =\n",
    "\\bf{c}\n",
    "\\end{equation}\n",
    "Note that for the matrix-vector to be well-defined, the length of the vector and the number of columns in the matrix must be the same. Looking at it a different way, the $n^{\\rm th}$ element of the vector $\\bf c$ is the dot product between the $n^{\\rm th}$ row of $\\bf A$ and the vector $\\bf b$.\n",
    "As an example, matrix vector product of\n",
    "\\begin{equation}\n",
    "{\\bf A} = \n",
    "\\begin{pmatrix}\n",
    "0 & 1  \\\\\n",
    "2 & 3  \\\\\n",
    "4 & 5\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}\n",
    "{\\bf b} = \n",
    "\\begin{pmatrix}\n",
    "6  \\\\\n",
    "7 \n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "is\n",
    "\\begin{equation}\n",
    "{\\bf c} = \n",
    "\\begin{pmatrix}\n",
    "0 & 1  \\\\\n",
    "2 & 3  \\\\\n",
    "4 & 5\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "6  \\\\\n",
    "7 \n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "0\\cdot 6 + 1\\cdot 7 \\\\\n",
    "2\\cdot 6 + 3\\cdot 7 \\\\\n",
    "4\\cdot 6 + 5\\cdot 7 \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "7  \\\\\n",
    "33 \\\\\n",
    "59\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "<br> <center> **MATRIX MATRIX PRODUCTS** </center> <br>\n",
    "\n",
    "The matrix product of an $N\\times K$ matrix $\\bf A$ and a $K\\times M$ matrix $\\bf B$ is an $N\\times M$ matrix $\\bf C$ with elements\n",
    "\\begin{equation}\n",
    "C_{n,m} = \\left( {\\bf AB} \\right)_{n,m} = \\sum_{k=1}^{K} A_{n,k}\\cdot B_{k,m}\n",
    "\\end{equation}\n",
    "Note that for the matrix product to be well-defined, the number of columns in matrix $\\bf A$ and the number of rows in matrix $\\bf B$ must be the same. \n",
    "As an example, the matrix product of \n",
    "\\begin{equation}\n",
    "{\\bf A} = \n",
    "\\begin{pmatrix}\n",
    "0 & 1 \\\\\n",
    "2 & 3\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}\n",
    "{\\bf B} = \n",
    "\\begin{pmatrix}\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "is given by\n",
    "\\begin{equation}\n",
    "{\\bf C = AB} =\n",
    "\\begin{pmatrix}\n",
    "0 & 1 \\\\\n",
    "2 & 3\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "0\\cdot 4 + 1\\cdot 7 & 0\\cdot 5 + 1\\cdot 8 & 0\\cdot 6 + 1\\cdot 9 \\\\\n",
    "2\\cdot 4 + 3\\cdot 7 & 2\\cdot 5 + 3\\cdot 8 & 2\\cdot 6 + 3\\cdot 9 \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "7  &  8 &  9 \\\\\n",
    "29 & 34 &  39\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "Matrix products are associative ($\\bf A(BC) = (AB)C$), distributive ($\\bf A(B+C) = AB + AC$), and in general (though not always) non-commutative ($\\bf AB \\neq BA$). Note that the identity matrix commutes with any other matrix $\\bf IA = AI$ and the product of the identiy matrix with another matrix is the matrix itself ($\\bf IA = A$). \n",
    "\n",
    "<br> <center> **DETERMINANTS** </center> <br>\n",
    "\n",
    "The detemrminant of an $N \\times N$ square matrix $\\bf A$, denoted ${\\rm det} ({\\bf A})$ or $\\vert {\\bf A} \\vert$, is a scalar and is the weighted sum of the determinants of the minors of $\\bf A$. The minor ${\\bf m}_{i,j}$ of $\\bf A$ is an $N-1 \\times N-1$ matrix obtained by removing the $i^{\\rm th}$ row and $j^{\\rm th}$ column from $\\rm A$. Specifically, for every row $i$ of $\\bf A$ the Laplace expansion is\n",
    "\\begin{equation}\n",
    "\\vert {\\bf A} \\vert = \\sum_{j=1}^N \\left( -1 \\right)^{i+j}A_{i,j}\\vert {\\bf d}_{i,j} \\vert .\n",
    "\\end{equation}\n",
    "As an example, the determinant of\n",
    "\\begin{equation}\n",
    "{\\bf A} =\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "is obtained by choosing the first row ($i=1$) and expanding as\n",
    "\\begin{equation}\n",
    "\\vert {\\bf A} \\vert =  \n",
    "\\begin{vmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{vmatrix} = \n",
    "\\left( -1 \\right)^{1+1} \\cdot 1 \\cdot \\begin{vmatrix} 5 & 6 \\\\ 8 & 9 \\end{vmatrix} + \\left( -1 \\right)^{1+2} \\cdot 2 \\cdot \\begin{vmatrix} 4 & 6 \\\\ 7 & 9 \\end{vmatrix} + \\left( -1 \\right)^{1+3} \\cdot 3 \\cdot \\begin{vmatrix} 4 & 5 \\\\ 7 & 8 \\end{vmatrix} = 1 \\cdot \\begin{vmatrix} 5 & 6 \\\\ 8 & 9 \\end{vmatrix} - 2 \\cdot \\begin{vmatrix} 4 & 6 \\\\ 7 & 9 \\end{vmatrix} + 3 \\cdot \\begin{vmatrix} 4 & 5 \\\\ 7 & 8 \\end{vmatrix}  \n",
    "\\end{equation}\n",
    "Next, we expand the remaining determinants to get the final result\n",
    "\\begin{equation}\n",
    "\\vert {\\bf A} \\vert = \n",
    "1 \\cdot \\begin{vmatrix} 5 & 6 \\\\ 8 & 9 \\end{vmatrix} - 2 \\cdot \\begin{vmatrix} 4 & 6 \\\\ 7 & 9 \\end{vmatrix} + 3 \\cdot \\begin{vmatrix} 4 & 5 \\\\ 7 & 8 \\end{vmatrix} = \n",
    "1 \\cdot \\left( 5 \\cdot 9 - 6 \\cdot 8 \\right) -2 \\cdot \\left( 4 \\cdot 9 - 6 \\cdot 7 \\right) + 3 \\cdot \\left( 4 \\cdot 8 - 5 \\cdot 7 \\right) = 0\n",
    "\\end{equation}\n",
    "\n",
    "Note that we get the same result if we expand along any of the rows or any of the columns of $\\bf A$. Furthermore, the sign of the determinant changes if one interchanges any two rows or any two columns of $\\bf A$. The determinant of a product of two matrices $\\bf A$ and $\\bf B$ is the product of the determinants of the matrices; $\\vert {\\bf AB}\\vert = \\vert {\\bf A}\\vert \\cdot \\vert {\\bf B}\\vert$. Lastly, the determinant of a diagonalizable matrix $\\bf A$ is equal to the product of the eigenvalues $\\{ e_n \\}$; $\\vert {\\bf A} \\vert = \\Pi_n e_n$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d6557-d807-47a6-b605-6f396b16f99d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc485349-9d0b-4dd9-bcb5-45474b783dbf",
   "metadata": {},
   "source": [
    "## 1. Overview of the linear variation method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9bede4-3308-4bdc-95bb-4c88ee178341",
   "metadata": {},
   "source": [
    "Analytical solutions to the time-independent Schrödinger equation\n",
    "\\begin{equation}\n",
    "\\hat{H} \\psi = E \\psi\n",
    "\\tag{1.1}\n",
    "\\end{equation}\n",
    "are known for a select few model problems (e.g. particle in a 1-D/2-D/3-D box, harmonic oscillator, particle on a ring/sphere, hydrogen atom and one-electron ions), and most problems encountered in quantum chemistry necessitate that we make some sort of approximation when solving the time-independent Schrödinger equation. \n",
    "\n",
    "One such approximation method, called the *linear variational method*, expresses the wave function $\\psi$ as a *linear combination* of *basis functions* $\\{ \\phi_n \\}$\n",
    "\\begin{equation}\n",
    "\\psi = \\sum_{n=1}^{N} c_n \\phi_n\n",
    "\\tag{1.2}\n",
    "\\end{equation}\n",
    "with *constant coefficients* $\\{ c_n \\}$. Put \"simply\", the basis functions are used to describe the wave function, and the importance of a basis function $\\phi_n$ in this description is proportional to the magnitude of the coefficient $c_n$. For pratical reasons, the number of basis functions $N$ is finite; thus, the wave function in Eq. (1.2) is, in general, approximate. Because the wave function depends on the coefficients, the energy also depends on the coefficients. Thus, to find the ground-state solution to the time-independent Schrödinger equation, our task is to determine the set of coefficients that minimize the energy. Once we find the optimal set of normalized coefficients, we can express the wave function as in Eq. (1.2) and calculate the approximate energy as $E_{approx}=\\int \\psi^* \\hat{H} \\psi {\\rm d}\\tau$ (where ${\\rm d} \\tau$ denotes a general volume element) and other expectation values of interest. For the ground state (and in some cases excited states) one can prove that the approximate energy is guaranteed to be an *upper bound* to the exact energy (i.e. $E_{approx} \\geq E_{exact}$ the approximate energy approaches the exact energy from above as the number of basis functions is increased).\n",
    "\n",
    " [Section 2](#general-approach-to-using-the-linear-variation-method) presents the general approach to applying the linear variation method. [Section 3](#a-note-on-atomic-units) introduces atomic units, and the basic syntax for matrix diagonalization is presented in [section 4](#matrix-diagonalization-in-python). In the [appendix](#appendix:-a-simple-example-to-demonstrate-the-application-of-the-linear-variation-method), we use a \"simple\" problem to demonstrate the application of the linear variation method. During this discussion, we highlight concepts such as the matrix representation of the Hamiltonian, matrix form of the time-independent Schrödinger equation, eigenvalues, eigenvectors, and matrix diagonalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec80e2fd-9243-41da-b2d5-5fa614b9cf84",
   "metadata": {},
   "source": [
    "## 2. General approach to applying the linear variation method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ea01b-87fe-4156-bff4-5af18367ba34",
   "metadata": {},
   "source": [
    "To obtain approximate solutions to the time-indpendent Schrödinger equation within the framework of the linear variation method, one can follow the following steps.\n",
    "\n",
    "1. Select a basis set $\\{\\phi_n\\}$ and determine the number of basis functions ($N$) to include in the calculation.\n",
    "   - The basis functions should obey the appropriate boundary conditions\n",
    "   - Although the basis functions do not necessarily have to be orthonormal, it helps when they are.\n",
    "\\begin{equation}\n",
    "\\int_\\text{all space} \\phi_n^*  \\phi_m {\\rm d}\\tau = \\delta_{m,n}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "2. Calculate the matrix representation of Hamiltonian (an $N\\times N$ matrix)\n",
    "\\begin{equation}\n",
    "\\bf H = T + V \\tag{3.1}\n",
    "\\end{equation}\n",
    ". In the case of an orthonormal basis, the matrix form of the time-independent Schrödinger equation is ${\\bf Hc} = E {\\bf c}$.\n",
    "   - Since the Hamiltonian is the sum of the kinetic ($\\hat{T}$) and potential ($\\hat{V}$) energy operators, computing the matrix representation the kinetic ($\\bf T$) and potential ($\\bf V$) energy matrices separately helps organize and debug your code.\n",
    "   - The kinetic energy matrix elements are given by\n",
    "\\begin{equation}\n",
    "T_{n,m} = \\int_\\text{ all space} \\phi_n^* \\hat{T} \\phi_m {\\rm d}\\tau\n",
    "\\tag{3.2}\n",
    "\\end{equation}\n",
    "   - The potential energy matrix elements are given by\n",
    "\\begin{equation}\n",
    "V_{n,m} = \\int_\\text{all space} \\phi_n^* \\hat{V} \\phi_m {\\rm d}\\tau\n",
    "\\tag{3.3}\n",
    "\\end{equation}\n",
    "   - Depending on the operator and the basis set, these integrals may be simplified. If they cannot, one can (hopefully) use some numerical integration and/or differentiation technique.\n",
    "     \n",
    "<br>\n",
    "\n",
    "3. Diagonalize the Hamiltonian to obtain the eigenvalues (approximate energies) and eigenvectors (expansion coefficients). There will be $N$ eigenvalues $\\{ E^{(n)}\\}$ and $N$ eigenvectors $\\{ {\\bf c}^{(n)}\\}$ (column vectors). Generally, the eigenvectors after diagonalization are orthonormal\n",
    "\n",
    "\\begin{equation}\n",
    "\\left({\\bf c}^{(n)}\\right)^T \\cdot {\\bf c}^{(m)} = \\delta_{m,n} \n",
    "\\end{equation}\n",
    "\n",
    "4. Using the expansion coefficients, the approximate wave function $\\psi^{(n)}$ corresponding to eigenvalue $E^{(n)}$ \n",
    "\\begin{equation}\n",
    "\\psi^{(n)} = \\sum_{i=1}^{N} c_i^{(n)} \\phi_i\n",
    "\\end{equation}\n",
    "can be used to calculate properties of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230865e9-a9c6-42bc-837e-7d8ef438dd03",
   "metadata": {},
   "source": [
    "## 3. A note on atomic units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607fbd6-2436-494a-8997-72cf64146aed",
   "metadata": {},
   "source": [
    "Because expressing properties (e.g. energies and distances) of atomic systems is quite cumbersome in SI units, most calculations on the atomic scale use *atomic units*. In atomic units,\n",
    "\\begin{equation}\n",
    "m_e = e = 4\\pi\\varepsilon_0 = \\hbar = a_0 = E_h = 1,\n",
    "\\end{equation}\n",
    "where $1 \\hspace{1mm} a_0 = \\frac{4\\pi\\varepsilon_0 \\hbar^2}{m_ee^2}=0.529177 $ ${\\mathring{\\rm{A}}}$ is the Bohr radius and $1 \\hspace{1mm} E_h = \\frac{\\hbar^2}{m_ea_0^2} = 27.2114$ eV is the Hartree. Note that although the mass of an electron is 1 in atomic units, the mass of a proton is much larger\n",
    "\\begin{equation}\n",
    "m_p^{(au)} = \\frac{m_p^{(kg)}}{m_e^{(kg)}} \\cdot m_e^{(au)} = \\frac{1.67262\\times10^{-27}}{9.10939\\times 10^{-31}} = 1836.15\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1609f5db-a450-457c-b092-ffc81e443804",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Matrix diagonalization in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276e8fd-799f-404a-b10e-24157204441a",
   "metadata": {},
   "source": [
    "As noted in sections 2 and 3, determining the ground-state energy and corresponding wave function requires the diagonalization of the Hamiltonian matrix. Python's scipy libraries offer a variety of functions to solve eigenvalue problems. Because the Hamiltonian matrix is symmetric and we want both the eigenvectors and eigenvalues, we will use the ```eigh``` function. The basic syntax for diagonalizing an $N\\times N$ matrix $\\bf M$ is\n",
    "\n",
    "```Python\n",
    "eig_vals , eig_vecs = scipy.linalg.eigh(M)\n",
    "```\n",
    "where, upon return, the vector ```eig_vals``` contains the $N$ eigenvalues of the matrix $\\bf M$ in **ascending order**, and the columns of the $N\\times N$ matrix ```eig_vecs``` correspond to the eigenvectors. That is, the eigenvector associated with the $i^{\\rm th}$ eigenvalue (```eig_vals[i]```) of $\\bf M$ is in the $i^{\\rm th}$ column of the matrix ```eig_vecs``` (```eig_vecs[ :, i ] ```). To find out more about the advanced options for ```eigh```, use ```help(scipy.linalg.eigh)```.\n",
    "\n",
    "To demonstrate the syntax, let's diagonalize the Hamiltonian matrix given in Eq. A.16 of the Appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e3c1fd-4757-43eb-97a6-5b2341d61148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State index= 0   Energy(eV)=  42.441  Eigenvector=(  -0.9993 ,  -0.0375 )\n",
      "State index= 1   Energy(eV)= 155.559  Eigenvector=(  -0.0375 ,   0.9993 )\n",
      "\n",
      "The overlap matrix between the eigenvectors is (within machine precision, this should be the identity matrix):\n",
      "\n",
      "[[1.00000000e+00 1.97694227e-18]\n",
      " [1.97694227e-18 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as spla\n",
    "\n",
    "# define Hamiltonian matrix based on the appendix\n",
    "H = np.array( [ [ 42.6, -4.24 ] , [ -4.24 , 155.4] ] )\n",
    "\n",
    "# initialize arrays/matrices to store eigenvalues and eigenvectors (the size of the Hamiltonian is 2 x 2, so N = 2)\n",
    "N = 2\n",
    "eig_vals = np.array( N )\n",
    "eig_vecs = np.array( ( N , N ) )\n",
    "\n",
    "# diagonalize the Hamiltonian matrix\n",
    "eig_vals , eig_vecs = spla.eigh( H )\n",
    "\n",
    "# define an array for printing results (strictly speaking, this is not necessary, \n",
    "# but it helps in demonstrating how to access eigenvectors\n",
    "vec_print = np.zeros( N )\n",
    "\n",
    "# print the eigenvalues and corresponding eigenvectors\n",
    "\n",
    "# loop over eigenpairs\n",
    "for i in range( N ):\n",
    "\n",
    "    e_print   = eig_vals[i]\n",
    "    vec_print = eig_vecs[ :, i ]\n",
    "    \n",
    "    print(F'State index={i:2d}   Energy(eV)={e_print:8.3f}  Eigenvector=( {vec_print[0]:8.4f} , {vec_print[1]:8.4f} )')\n",
    "\n",
    "# demonstrate the eigenvectors are orthonormal\n",
    "\n",
    "S_matrix = np.array( ( N , N ) )\n",
    "S_matrix = np.matmul( eig_vecs, np.transpose( eig_vecs ) )\n",
    "\n",
    "print()\n",
    "print(\"The overlap matrix between the eigenvectors is (within machine precision, this should be the identity matrix):\")\n",
    "print()\n",
    "print(S_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b005691b-7ca5-4950-9eea-f59950e34773",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> 5. Live coding exercise </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435852e8-544f-4e23-8206-c1471b094143",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "\n",
    "In this exercise, we will consider a particle with mass $M$ that moves along $x$ and experiences the potential\n",
    "\\begin{equation}\n",
    "V(x) = \\begin{cases}\n",
    " 0 \\hspace{2.1cm} 0 \\leq x \\leq L/2 \\\\\n",
    " \\\\\n",
    " V_1 \\hspace{2cm} L/2 \\leq x \\leq L \\\\\n",
    " \\\\\n",
    " \\infty \\hspace{2cm} \\text{everywhere else}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "where $V_1 > 0$. Using the orthonormal particle in 1-D box eigenfunctions\n",
    "\\begin{equation}\n",
    "\\phi_n(x) = \\sqrt{\\frac{2}{L}} {\\rm sin}\\left( \\frac{n\\pi x}{L}\\right) \\hspace{1cm} n = 1,2, ...\n",
    "\\end{equation}\n",
    "as our basis functions, we will demonstrate how to implement the linear variational method. To do so, we will write code that\n",
    "\n",
    "1. builds the kinetic energy matrix (see Eq. 3.2 and pre-lab exercise 4)\n",
    "2. builds the potential energy matrix (see Eq. 3.3 and pre-lab exercise 7)\n",
    "3. builds the Hamiltonian from the kinetic and potential energy matrices (see equation 3.1)\n",
    "4. calculates the approximate solutions (energies and expansion coefficients) of the time-independent Schrödinger equation by diagonalizing the Hamiltonian (see [section 4](#matrix-diagonalization-in-python))\n",
    "5. calculates the value of the ground-state wave function at some value of $x$ (see section 12 of the [\"Introduction to programming and Python\"](https://github.com/act-cms/lesson-01-introduction-to-programming-and-python) exercises)\n",
    "6. Prints important information and key results including the input parameters ($N$, $W$s, $V$s, etc ...) and the ground-state energy.\n",
    "\n",
    "In our algorithm, we will\n",
    "\n",
    "- keep $M$, $L$, $V_1$, and $N$ (the number of basis functions in the linear expansion in Eq. (1.2)) as input variables that the user can specify\n",
    "- assume that the user will input distances in units of  ${\\mathring{\\rm{A}}}$, energies in units of eV, and masses in units of kg\n",
    "- use atomic units throughout the calculations\n",
    "- to simplify debugging and to increase readability of your code, use functions as appropriate\n",
    "\n",
    "Once we have implemented our algorithm, we will demonstrate approaches to check our results are sensible and that our code gives the correct results for limiting cases (e.g. does our algorithm give the right result in the limit when $V_1 = 0$ or when $V_1$ is very large?). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e4e8d-c21d-4e7f-995f-18e93546ab5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "342ce679-9c90-47cd-b768-975291533f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b5793e6-4070-4926-a7a4-e9adfc403da1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define input variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7b96e623-18ca-4743-80a9-712c6036466a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dfe9d5c-7737-4a39-a56a-0a00da46c8c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2040430-c235-493b-93ca-ba90d8aa418a",
   "metadata": {},
   "source": [
    "In this section, we will add as many dropdown cells as needed to implement the functions that are going to be used in the code. \n",
    "\n",
    "__NOTE:__ To aid in reading and debugging your code, use a separate cell for each function! Lastly, to aid the end-user, make sure to include comments and documentation in the form of DocStrings for your functions. For a refresher on how to use DocStrings, see section 8 of the [\"Introduction to programming and Python\"](https://github.com/act-cms/lesson-01-introduction-to-programming-and-python) exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f0aa95cd-be0b-49cb-b878-327c2a04b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, write code for a function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "61070e9f-36ae-4c79-9f54-14b36942435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, write code for a function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "15e60848-e81e-4742-bcee-cfb2c0ad5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, write code for a function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fbf81e9c-c447-40df-a985-86f130d0ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, write code for a function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5a7172ad-930e-44b4-a202-504bdb0d622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, write code for a function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e83f7cce-7e43-4c1f-b6ee-4d5c78982595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, write code for a function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b8824abf-2d4c-420e-bfb0-f7248fa6056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, write code for a function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "34be418b-49c3-4f8e-979d-c2644f07bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, write code for a function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1fa43706-2bc2-4561-8ea7-624b6e6934ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, write code for a function here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314e0d2-388e-4917-b842-c1fc90fc728c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Initialize arrays and matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a1c28c48-71e3-4441-a82d-c3e203479e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "060ffb26-4fbf-4a79-a517-45c18aead6ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Convert parameter values to atomic units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d23fd49d-db48-4117-a3ca-098b8f7f6d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f305deb3-59a1-4899-8e9a-14a5e81f6f04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Calculate the kinetic energy, potential energy, and finally, the Hamiltonian matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "418befb4-977f-4c33-87ee-a9747a6503c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e31f8a4b-d8cf-4b0a-916d-c55bfbcb8e8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DIagonalize the Hamiltonian to calculate the approximate solutions to the Schrödinger equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fd68f814-566b-4131-b757-c4e609e27d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27c237ac-886a-4741-a3aa-343854218051",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Print key results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dadbf8b-580c-4027-955e-b5c8ae0d01e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5f6a2bf-f562-4582-b7a7-9aa1533365bc",
   "metadata": {},
   "source": [
    "## Appendix: A simple example to demonstrate the application of the linear variation method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a17219-2c74-4c53-87e7-88cd4e32b508",
   "metadata": {},
   "source": [
    "To simplify the notation, we will **assume that the coefficients, basis functions, and hence the wave function are real** for the remainder of this section. In most numerical applications, this turns out to be the case. Suppose we are interested in solving the time-independent Schrödinger equation (Eq. (1.1)) for an electron that experiences the potential\n",
    "\\begin{equation}\n",
    "V(x) = \\begin{cases}\n",
    " 0 \\hspace{2.1cm} 0 \\leq x \\leq L/2 \\\\\n",
    " \\\\\n",
    " V_0 \\hspace{2cm} L/2 \\leq x \\leq L \\\\\n",
    " \\\\\n",
    " \\infty \\hspace{2cm} \\text{everywhere else}\n",
    "\\end{cases} \\tag{A.1}\n",
    "\\end{equation}\n",
    "\n",
    "This potential effectively confines the particle to $0 \\leq x \\leq L$ and the electron experiences a higher potential in the right half of the box. Although the potential looks deceptively simple, the analytical solutions to Eq. (1.1) are not known. Below, we present a detailed discussion of obtaining the approximate solutions using the linear variation method.\n",
    "\n",
    "To start, we select a set of basis functions; because the potential in Eq. (A.1) is similar to the particle in a 1-D particle in a box potential, we will choose the eigenfunctions of the particle in a 1-D box Hamiltonian\n",
    "\\begin{equation}\n",
    "\\phi_n(x) = \\sqrt\\frac{2}{L}{\\rm sin}\\left( \\frac{n\\pi x}{L}\\right)\n",
    "\\tag{A.2}\n",
    "\\end{equation}\n",
    "as our basis functions. Although other choices are also possible (as long as they obey the right boundary conditions), the basis functions in Eq. (A.2) are convenient since they are orthonnormal (you showed this in section 13 of the [\"Introduction to programming and Python\"](https://github.com/act-cms/lesson-01-introduction-to-programming-and-python) exercises) on the domain $0 \\leq x \\leq L$. That is,\n",
    "\n",
    "\\begin{equation}\n",
    "\\int_0^L \\phi_n(x)^*  \\phi_m(x) {\\rm d}x = \\delta_{m,n}\n",
    "\\tag{A.3}\n",
    "\\end{equation}\n",
    "\n",
    "where the Krönecker delta function $\\delta_{m,n}$ is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "\\delta_{n,m}= \\begin{cases}\n",
    " 0 \\hspace{2.0cm} n \\neq  m\\\\\n",
    " \\\\\n",
    " 1 \\hspace{2cm} n = m\\\\\n",
    "\\end{cases}. \n",
    "\\end{equation}\n",
    "\n",
    "Having chosen the basis set, we need to decide how many basis functions to consider when describing the wave function. For simplicity, we will use two functions ($N=2$), and thus our wave function has the form\n",
    "\\begin{equation}\n",
    "\\psi(x) = c_1 \\phi_1(x) + c_2 \\phi_2(x) .\n",
    "\\tag{A.4}\n",
    "\\end{equation}\n",
    "\n",
    "To determine the equations for the coefficients that minimize the energy, we first multiply Eq. 1.1 from the left by $\\psi(x)^*$ and integrate\n",
    "\\begin{equation}\n",
    "\\int_{0}^{L} \\psi(x)^* \\hat{H} \\psi(x) dx = E \\int_{0}^L \\psi(x)^* \\psi(x) dx\n",
    "\\tag{A.5}\n",
    "\\end{equation}\n",
    "Note that we did not assume that $\\psi$ is normalized and kept the integral on the right hand side. Inserting Eq. (A.4) into Eq. (A.5), recognizing that the basis functions are orthonormal (as in Eq. (A.3)), and assuming that the coefficients are real, we have\n",
    "\\begin{equation}\n",
    "H_{1,1}c_1^2 + H_{1,2} c_1c_2 + H_{2,1}c_1c_2 + H_{2,2}c_2^2 = E \\left ( c_1^2 + c_2^2\\right),\n",
    "\\tag{A.6}\n",
    "\\end{equation}\n",
    "where the Hamiltonian matrix element $H_{n,m}$ is defined as\n",
    "\\begin{equation}\n",
    "H_{n,m} = \\int_{0}^L \\phi_n(x) \\hat{H} \\phi_m(x){\\rm d}x .\n",
    "\\tag{A.7}\n",
    "\\end{equation}\n",
    "Note that $H_{n,m}$ is independent of the coefficients and can be evaluated once the form of $\\hat{H}$ and $\\phi_m$ are known. \n",
    "Finally, since the energy is stationary with respect to variations in the coefficients $\\left( \\text{i.e.} \\frac{\\partial E}{\\partial c_1} = \\frac{\\partial  E}{\\partial c_2} = 0\\right) $ at the minimum, differentiating Eq. (A.6) with respect to $c_1$ and $c_2$ and recognizing that $H_{1,2} = H_{2,1}$ gives the two equations\n",
    "\\begin{equation}\n",
    "H_{1,1} c_1 + H_{1,2}c_2 = Ec_1 \n",
    "\\tag{A.8a}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "H_{2,1} c_1 + H_{2,2}c_2 = Ec_2 \n",
    "\\tag{A.8b}\n",
    "\\end{equation}\n",
    "In principle, one could solve this system of two equations by substitution, to get a quadratic equation for the energy $E$. Once the two roots are found, the coefficients corresponding to each root can be found from Eqs. (A.8a) and (A.8b).\n",
    "\n",
    "However, this process becomes impractical very quickly for $N>2$ and we take a different approach. Notice that we can succinctly represent Eqs. (A.8a) and (A.8b) as one *matrix equation*\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "H_{1,1} & H_{1,2} \\\\\n",
    "H_{2,1} & H_{2,2} \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "c_1 \\\\\n",
    "c_2\n",
    "\\end{pmatrix}\n",
    "= E\n",
    "\\begin{pmatrix}\n",
    "c_1 \\\\\n",
    "c_2\n",
    "\\end{pmatrix}\n",
    "\\iff {\\bf H}{\\bf c} = E{\\bf c}\n",
    "\\tag{A.9} \n",
    "\\end{equation}\n",
    "Equation (A.9) is referred to as the *matrix form* of the time-independent Schrödinger equation in an orthonormal basis and $\\bf H$ is called the matrix representation of the Hamiltonian (or Hamiltonian matrix for short) in the chosen basis set. The vector ${\\bf c}$ contains the expansion coefficients and is referred to as the eigenvector of ${\\bf H}$. $E$ is the eigenvalue that corresponds to the eigenvector ${\\bf c}$. Collectively, ${\\bf c}$ and $E$ are called an eigenpair.\n",
    "\n",
    "Now, to solve for the coefficients, we multiply the right side of Eq. (A.9) by the identity matrix ${\\bf I} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1\\end{pmatrix}$ and subtract the result from both sides to give\n",
    "\\begin{equation}\n",
    "\\left( {\\bf H} - E{\\bf I} \\right) {\\bf c} = {\\bf 0}\n",
    "\\tag{A.10}\n",
    "\\end{equation}\n",
    "The trivial solution ${\\bf c} = {\\bf 0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ is not interesting since it implies that $\\psi = 0$. The more interesting solution is obtained by considering properties of ${\\bf H} - E{\\bf I}$. If it is invertible $\\left( \\text{i.e.} \\left( {\\bf H} - E{\\bf I}\\right)^{-1} \\left({\\bf H} - E{\\bf I}\\right) = {\\bf I} \\right)$, multiplying Eq. (A.10) from the left by $\\left({\\bf H} - E{\\bf I}\\right)^{-1}$ implies that $\\bf c = 0$\n",
    "\\begin{equation}\n",
    "\\left( {\\bf H} - E{\\bf I}\\right)^{-1} \\left({\\bf H} - E{\\bf I}\\right) {\\bf c} = \\left( {\\bf H} - E{\\bf I}\\right)^{-1} {\\bf 0} \\hspace{1cm}\\Rightarrow\\hspace{1cm} {\\bf I}{\\bf c} = {\\bf 0} \\hspace{1cm}\\Rightarrow\\hspace{1cm}  {\\bf c} = {\\bf 0}\n",
    "\\end{equation}\n",
    "Thus, the inverse of ${\\bf H} - E{\\bf I}$ must not exist for non-trivial solutions (i.e. ${\\bf c} \\neq {\\bf 0}$). This condition is met, if the determinant of ${\\bf H} - E{\\bf I}$ vanishes\n",
    "\\begin{equation}\n",
    "\\vert {\\bf H} - E{\\bf I} \\vert = 0\n",
    "\\tag{A.11}\n",
    "\\end{equation}\n",
    "\n",
    "Expanding the determinant and solving for the roots of the resulting polynomial gives the eigenvalues of ${\\bf H}$. Then, for each eigenvalue, we can determine the associated eigvector ${\\bf}$ from Eq. (A.9). \n",
    "\n",
    "To demonstrate this process, we first calculate the Hamiltonian matrix elements using Eqs (A.1), (A.2), and (A.7). Since the Hamiltonian operator is the sum of the kinetic energy operator ($\\hat{T}_x$) and the potential energy opertor, the matrix representation of the Hamiltonian is $\\bf H = T + V$. Since the basis functions in Eq. (A.2) are orthonormal eigenfunctions of the particle in a 1-D box Hamiltonian (for which $\\hat{H} = \\hat{T}_x = - \\frac{\\hbar^2}{2m}\\frac{d^2}{dx^2}$) operator, the elements of the kinetic energy matrix are \n",
    "\\begin{equation}\n",
    "T_{n,m} = m^2 \\left(\\frac{\\hbar^2\\pi^2}{2m_eL^2}\\right) \\delta_{n,m}.\n",
    "\\tag{A.12}\n",
    "\\end{equation}\n",
    "\n",
    "Since the value of the Krönecker delta function $\\delta_{n,m}$ vanishes unless $n=m$ and is equal to 1 when $n=m$, we see that $\\bf T$ is a diagonal matrix. For an electron with mass $m_e$ in a box of width $L=1$ ${\\mathring{\\rm{A}}}$, $ \\left(\\frac{\\hbar^2\\pi^2}{2m_eL^2}\\right) = 37.6$ eV, and thus, $\\bf T$ (in units of eV) has the form\n",
    "\\begin{equation}\n",
    "{ \\bf T} =\n",
    "\\begin{pmatrix}\n",
    "37.6 & 0 \\\\\n",
    "0  & 150.4\n",
    "\\end{pmatrix}\n",
    "\\tag{A.13}\n",
    "\\end{equation}\n",
    "Now, since $V(x)=0$ for $0\\leq x \\leq L/2$ and $V = V_0$ for $L/2 \\leq x \\leq L$, the potential energy matrix element\n",
    "\\begin{equation}\n",
    "V_{n,m} = \\int_0^L \\phi_n(x)\\hat{V}(x)\\phi_m(x) {\\rm d}x = \\int_{L/2}^L \\phi_n(x)V_0\\phi_m(x) {\\rm d}x = V_0\\int_{L/2}^L \\phi_n(x)\\phi_m(x) {\\rm d}x\n",
    "\\tag{A.14}\n",
    "\\end{equation}\n",
    "requires that we evaluate the integral in Eq. (A.14). For this case, this can be done by hand, however, one needs to use numerical integration techniques for the general case. For  $V_0 = 10$ eV, the potential energy (in units of eV) is\n",
    "\\begin{equation}\n",
    "{ \\bf V} =\n",
    "\\begin{pmatrix}\n",
    "5.0 & -4.24 \\\\\n",
    "-4.24  & 5.0\n",
    "\\end{pmatrix}\n",
    "\\tag{A.15}\n",
    "\\end{equation}\n",
    "Thus, the Hamiltonian matrix is\n",
    "\\begin{equation}\n",
    "{ \\bf H} =\n",
    "\\begin{pmatrix}\n",
    "42.6 & -4.24 \\\\\n",
    "-4.24 & 155.4\n",
    "\\end{pmatrix}\n",
    "\\tag{A.16}\n",
    "\\end{equation}\n",
    "and hence ${\\bf H} - E {\\bf I}$ is given by\n",
    "\\begin{equation}\n",
    "{ \\bf H} - E{\\bf I} =\n",
    "\\begin{pmatrix}\n",
    "42.6 & -4.24 \\\\\n",
    "-4.24 & 155.4\n",
    "\\end{pmatrix}\n",
    "- E\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "42.6 - E & -4.24 \\\\\n",
    "-4.24 & 155.4 - E \n",
    "\\end{pmatrix}\n",
    "\\tag{A.17}\n",
    "\\end{equation}\n",
    "To find the eigenvalues $E$, we expand the determinant\n",
    "\\begin{equation}\n",
    "\\vert {\\bf H} - E{\\bf I} \\vert = 0\n",
    "\\hspace{0.5cm}\\Rightarrow\\hspace{0.5cm}\n",
    "\\begin{vmatrix}\n",
    "42.6 -E & -4.24 \\\\\n",
    "-4.24 & 155.4 - E\n",
    "\\end{vmatrix} =0\n",
    "\\hspace{0.5cm}\\Rightarrow\\hspace{0.5cm}\n",
    "(42.6 - E)(155.4 - E) - (-4.24)^2 = 0\n",
    "\\hspace{0.5cm}\\Rightarrow\\hspace{0.5cm}\n",
    "E^2 - 198.0 E + 6602.1 = 0\n",
    "\\tag{A.18}\n",
    "\\end{equation}\n",
    "and solve for the roots of the quadratic equation; the two solutions (in units of eV) are $E^{(1)} = 42.441$ and $E^{(2)} = 155.56$. These are the eigenvalues of the Hamiltonian matrix and correspond to the allowed energies for an electron that experiences the potential given in Eq. (A.1).\n",
    "\n",
    "Lastly, we use these eigenvalues to determine the eigenvectors of ${\\bf H}$. First, we insert $E^{(1)} = 42.441$ into the matrix form of the Schrödinger equation as in Eq. (A.9) with the Hamiltonian given in Eq. (A.16)\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "42.6 & -4.24 \\\\\n",
    "-4.24 & 155.4 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "c_1^{(1)} \\\\ c_2^{(1)}\n",
    "\\end{pmatrix} \n",
    "= 42.441\n",
    "\\begin{pmatrix}\n",
    "c_1^{(1)} \\\\ c_2^{(1)}\n",
    "\\end{pmatrix}\n",
    "\\hspace{0.5cm}\\Rightarrow\\hspace{0.5cm}\n",
    "\\begin{pmatrix}\n",
    "42.6 c_1^{(1)}  -  4.24 c_2^{(1)} \\\\\n",
    "-4.24 c_1^{(1)} + 155.4 c_2^{(1)} \n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "42.441 c_1^{(1)} \\\\ 42.441 c_2^{(1)}\n",
    "\\end{pmatrix}\n",
    "\\tag{A.19}\n",
    "\\end{equation}\n",
    "The matrix equation (2.19) is equivalent two coupled equations, the first one being $42.6 c^{(1)}_1  -  4.24 c_2^{(1)} = 42.441 c_1^{(1)}$. Solving for $c_1^{(1)}$ in terms of $c_2^{(1)}$, we get \n",
    "\\begin{equation}\n",
    "c_1^{(1)} = 26.667 c_2^{(1)}\n",
    "\\tag{A.20}\n",
    "\\end{equation}\n",
    "Since the basis functions $\\phi_1(x)$ and $\\phi_2(x)$ are orthonormal (see Eq. (A.3)), one can show that for a normalized wave function $\\int_0^L \\vert \\psi(x)  \\vert^2 {\\rm d}x = 1$, the coefficients $c_1^{(1)}$ and $c_2^{(1)}$ are related by\n",
    "\\begin{equation}\n",
    "\\left(c_1^{(1)}\\right)^2 + \\left(c_2^{(1)}\\right)^2 = 1\n",
    "\\tag{A.21}\n",
    "\\end{equation}\n",
    "Inserting Eq. (2.20) into Eq. (2.21) gives\n",
    "\\begin{equation}\n",
    "\\left( 3.65833 c_2^{(1)} \\right)^2 + \\left(c_2^{(1)}\\right)^2 = 1\n",
    "\\hspace{0.5cm}\\Rightarrow\\hspace{0.5cm}\n",
    "\\left( c_2^{(1)}\\right) ^2 = 1.404\\times10^{-3} \n",
    "\\hspace{0.5cm}\\Rightarrow\\hspace{0.5cm}\n",
    "c_2^{(1)} = \\pm 0.03747 \n",
    "\\tag{A.22}\n",
    "\\end{equation}\n",
    "Because it is the relative sign of $c_1^{(1)}$ and $c_2^{(1)}$ that matters, we can pick either solution in Eq. (A.22). Using the positive solution, we get $c_1^{(1)} = 0.99860$. Thus, the first eigenpair is given by\n",
    "\\begin{equation}\n",
    "E^{(1)}=42.441 \\hspace{1cm}\\&\\hspace{1cm} {\\bf c}^{(1)} = \n",
    "\\begin{pmatrix}\n",
    "0.99860 \\\\ 0.03747\n",
    "\\end{pmatrix}\n",
    "\\hspace{1cm}\\Rightarrow\\hspace{1cm} \\psi^{(1)}(x) = 0.99860\\cdot \\phi_1(x) + 0.03747\\cdot \\phi_2(x)\n",
    "\\tag{A.23}\n",
    "\\end{equation}\n",
    "Using similar logic, we get the second eigenpair\n",
    "\\begin{equation}\n",
    "E^{(2)}=155.56 \\hspace{1cm}\\&\\hspace{1cm} {\\bf c}^{(2)} = \n",
    "\\begin{pmatrix}\n",
    "- 0.03747 \\\\  0.99860\n",
    "\\end{pmatrix}\n",
    "\\hspace{1cm}\\Rightarrow\\hspace{1cm} \\psi^{(2)}(x) = - 0.03747\\cdot \\phi_1(x) + 0.99860\\cdot \\phi_2(x)\n",
    "\\tag{A.24}\n",
    "\\end{equation}\n",
    "\n",
    "Inspection of the Hamiltonian matrix in Eq. (A.16) reveals that it is symmetric since $H_{1,2} = H_{2,1}$. In general, Hamiltonian matrices with real elements are always symmetric and $H_{m,n} = H_{n,m}$. As it turns out, finding the eigenvectors of a matrix is equivalent to diagonalizing a matrix. Diagonalization is defined as finding the similarity transformation matrix ${\\bf P}$ that converts the matrix $\\bf A$ to a diagonal matrix ${\\bf D} = {\\bf P}^T {\\bf A}{\\bf P}$ where the columns of $\\bf P$ correspond to the eigenvectors of the matrix $\\bf A$ and $^T$ denotes the transpose (obtained by interchanging colum and row indeces) of a matrix. To demonstrate this, using the two eigenvectors found in Eqs. (A.23) and (A.24), we have\n",
    "\\begin{equation}\n",
    "{\\bf P} =  \n",
    "\\begin{pmatrix} {\\bf c}^{(1)} & {\\bf c}^{(2)} \\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "0.99860 & - 0.03747 \\\\\n",
    "0.03747 &   0.99860\n",
    "\\end{pmatrix}\n",
    "\\hspace{1cm}\\Rightarrow\\hspace{1cm} \n",
    "{\\bf P}^T =\n",
    "\\begin{pmatrix}\n",
    "  0.99860 &   0.03747 \\\\\n",
    "- 0.03747 &   0.99860\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "For the Hamiltonian in Eq. (A.16) we see that\n",
    "\\begin{eqnarray}\n",
    "{\\bf D} = {\\bf P}^T{\\bf H}{\\bf P} &=&\n",
    "\\begin{pmatrix}\n",
    "  0.99860 &   0.03747 \\\\\n",
    "- 0.03747 &   0.99860\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "  42.6 & -4.24 \\\\\n",
    "-4.24 &  155.4 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "0.99860 & - 0.03747 \\\\\n",
    "0.03747 &   0.99860\n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "&=&\n",
    "\\begin{pmatrix}\n",
    "  0.99860 &   0.03747 \\\\\n",
    "- 0.03747 &   0.99860\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "42.382 & -5.830 \\\\\n",
    "1.589 &  155.34\n",
    "\\end{pmatrix} \n",
    "\\\\\n",
    "&=&\n",
    "\\begin{pmatrix}\n",
    "42.382 &  -0.001 \\\\\n",
    " -0.001 & 155.34\n",
    "\\end{pmatrix} \n",
    "\\end{eqnarray}\n",
    "\n",
    "As demonstrated in the cell below (go ahead, and run the code yourself), the fact that the off-diagonal elements of the above matrix are not exactly zero and that the diagonal elements are slightly different from the eigenvalues of ${\\bf H}$ *is the result of rounding*. Nonetheless, this example demonstrates that finding the matrix $\\bf P$ that brings $\\bf H$ to diagonal form is equivalent to finding the eigenvalues and eigenvectors or $\\bf H$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a490d92-8aab-469a-bb28-146a9d4aee96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E1= 42.44084866 c_1=  0.99929628 c_2=  0.03750928\n",
      "E2=155.55915134 c_1= -0.03750928 c_2=  0.99929628\n",
      "\n",
      "P = \n",
      "[[0.9992962794157921, -0.03750927812895497], [0.03750927812895414, 0.9992962794157921]]\n",
      "\n",
      "P^T H P =\n",
      "[[ 4.24408487e+01 -1.29064543e-13]\n",
      " [-1.29269324e-13  1.55559151e+02]]\n",
      "\n",
      "eigenvalues after diagonalization\n",
      "[ 42.44084866 155.55915134]\n",
      "\n",
      "eigenvectors after diagonalization (note that overall phases might differ from the ones above)\n",
      "[[-0.99929628  0.03750928]\n",
      " [-0.03750928 -0.99929628]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "def ratio_value(H_11,H_12,E):\n",
    "    return H_12/(E-H_11)\n",
    "\n",
    "def c2_value(ratio):\n",
    "    return np.sqrt(1.0/(1.0+ratio**2))\n",
    "\n",
    "def c1_value(ratio,c2):\n",
    "    return ratio * c2\n",
    "\n",
    "def root_value(a,b,c,root):\n",
    "    print(a*root**2+b*root+c)\n",
    "\n",
    "# H matrix elements\n",
    "H_11 = 42.6\n",
    "H_12 = -4.24\n",
    "H_22 = 155.4\n",
    "\n",
    "# coefficients in quadratic equation\n",
    "a    = 1.0\n",
    "b    = -(H_11+H_22)\n",
    "c    = +H_11*H_22-(H_12**2)\n",
    "\n",
    "# roots of quadratic equation\n",
    "E1   = ( - b - np.sqrt(b**2 - 4.0*a*c) ) / ( 2.0 * a )\n",
    "E2   = ( - b + np.sqrt(b**2 - 4.0*a*c) ) / ( 2.0 * a )\n",
    "\n",
    "print(\"\")\n",
    "# solve for coefficients for eigenvalue E1\n",
    "ratio_1 = ratio_value(H_11,H_12,E1)\n",
    "c_21    = c2_value(ratio_1)\n",
    "c_11    = c1_value(ratio_1,c_21)\n",
    "print(F'E1={E1:12.8f} c_1={c_11:12.8f} c_2={c_21:12.8f}')\n",
    "\n",
    "# solve for coefficients for eigenvalue E2\n",
    "ratio_2 = ratio_value(H_11,H_12,E2)\n",
    "c_22    = c2_value(ratio_2)\n",
    "c_12    = c1_value(ratio_2,c_22)\n",
    "print(F'E2={E2:12.8f} c_1={c_12:12.8f} c_2={c_22:12.8f}')\n",
    "\n",
    "# define Hamiltonian matrix\n",
    "H = [[H_11,H_12],[H_12,H_22]]\n",
    "# construct similarity transformation matrix P \n",
    "P = [ [c_11 , c_12] , [c_21 , c_22] ]\n",
    "print(\"\")\n",
    "print(\"P = \")\n",
    "print(P)\n",
    "\n",
    "#bring Hamiltonian to diagonal form\n",
    "print(\"\")\n",
    "print(\"P^T H P =\")\n",
    "print(np.matmul(np.transpose(P),np.matmul(H,P)))\n",
    "\n",
    "# much much easier to do with Python's diagonalization routine\n",
    "eig,vec = sp.linalg.eig(H)\n",
    "\n",
    "print(\"\")\n",
    "print(\"eigenvalues after diagonalization\")\n",
    "print(np.real(eig))\n",
    "\n",
    "print(\"\")\n",
    "print(\"eigenvectors after diagonalization (note that overall phases might differ from the ones above)\")\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d743741-6b3f-47e3-a73e-dd6f31999670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
